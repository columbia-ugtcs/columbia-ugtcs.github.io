<!DOCTYPE html>
<html lang="en" class="no-js">

<head>
    <!--Website Google searchability-->
    <meta name="google-site-verification" content="xvD194KsBxeaW40tr48z5nGiEF-zbgEgpTNHhK2AMu8" />
    <!-- Mobile Specific Meta -->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- Favicon-->
    <link rel="shortcut icon" href="img/fav.png">
    <!-- Author Meta -->
    <meta name="author" content="Clayton">
    <!-- Meta Description -->
    <meta name="description" content="">
    <!-- Meta Keyword -->
    <meta name="keywords" content="">
    <!-- meta character set -->
    <meta charset="UTF-8">
    <!-- Site Title -->
    <title>Columbia Undergraduate Learning Seminar in Theoretical Computer Science</title>
    <link rel="stylesheet" href="undergrad-seminar-style.css">
    <!--Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Istok+Web|Roboto+Slab&display=swap" rel="stylesheet">
    <!--Favicon-->
    <link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
    <link rel="manifest" href="favicon/site.webmanifest">
    <link rel="mask-icon" href="favicon/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    <!-- Script -->
    <script type="text/javascript">
    function toggle_visibility(id) {
        var e = document.getElementById(id);
        if (e.style.display == 'block')
            e.style.display = 'none';
        else
            e.style.display = 'block';
    }
    //-->
    </script>
</head>

<body>
    <h1>Columbia Undergraduate Learning Seminar in Theoretical Computer Science</h1>
   
    <h2>About</h2>
    <p>The Columbia Undergraduate Learning Seminar in Theoretical Computer Science is a student-run seminar for undergraduates at Columbia interested in theoretical computer science. The goal of the learning seminar is to provide undergraduate students with the opportunity to learn about theoretical computer science in a collaborative, student-driven setting and to meet other students interested in theoretical computer science.
    </p>
    <p>
    The learning seminar is dedicated to providing an inclusive and welcoming environment for all students interested in theoretical computer science. No background in theoretical computer science is required to participate in the seminar, and everyone is welcome to join!
    </p>   
    <p>
    Each semester, the Columbia Undergraduate Learning Seminar in Theoretical Computer Science will hold one or more seminars on topics related to TCS. The presentations will primarily be given by students, which is a great opportunity to gain experience giving a technical talk in TCS and meet other students interested in the topic.
    </p>
    <hr>
    <h2>Join Us!</h2>
    <p>The seminar is currently run by Ekene Ezeunala. If you have any questions or would like to join the seminar's Slack channel, please email him <a href="mailto:efe2110@columbia.edu?subject=%5BUndergrad TCS Seminar%5D">here</a>.
    </p>
    <p>Sign up for the mailing list here: <a href="https://forms.gle/8peARsv9VyQMz3wz7">Mailing List Sign-Up</a>.
    </p>
   

    <hr>
    <h2> Spring 2024
    </h2>
    <div id='spring24' style="display: block;">
        <p>This Spring semester, we will be holding three groups, each focused on a different topic within TCS. The groups are: Neural Manifolds (and TCS), Quantum Complexity Theory, and Randomness in Computation. Each group is run by an undergraduate student organizer along with a graduate student mentor. The groups meet roughly weekly and should be approachable for students of all ranges of prior exposure to TCS.</p>
        <p>Please see the descriptions and tables below for a summary and the list of talks for each of the groups.</p>

        <div class='spring24-group'>
            <h3>Neural Manifolds and TCS</h3>
            <p>Organizer: Shujun. Graduate Student Mentor: Todd.</p>
            <p><b>Description:</b> This seminar aims to ground geometric conceptions of learning with learning in the brain. Manifolds are a key structure to unify these perspectives, and much of this seminar will aim to build towards understanding how they work. The first half of this seminar will build key concepts in learning over geometry, while the second half aims to use these tools to study learning in the brain.</p>
            <div style="height: 10px;"></div>
            <table>
                <tr>
                    <th>Resource</th>
                    <th>Title</th>
                    <th>Link</th>
                </tr>
                <tr>
                    <td>Gerstner</td>
                    <td>Neuronal Dynamics</td>
                    <td><a href="https://neuronaldynamics.epfl.ch/online/index.html">Link</a></td>
                </tr>
            </table>
            <div style="height: 30px;"></div>
            <table>
                <tr>
                    <th>Date</th>
                    <th>Topic</th>
                    <th>Speaker</th>
                    <th>Reading</th>
                </tr>
                <tr>
                    <td>February 23rd</td>
                    <td>Introduction</td>
                    <td>Shujun</td>
                    <td><a href="https://arxiv.org/abs/2304.06636">Gabrie et al (2023), “Neural networks: from the perceptron to deep nets.”</a></td>
                </tr>
                <tr>
                    <td>March 2nd</td>
                    <td>Sparse Coding</td>
                    <td>Shujun</td>
                    <td><a href="https://ganguli-gang.stanford.edu/pdf/12.CompSense.pdf">S. Ganguli & H. Sompolinsky (2012), "Compressed Sensing, Sparsity, and Dimensionality in Neuronal Information Processing and Data Analysis.”</a>
                        <br><a href="https://home.ttic.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf">S. Kakade and G. Shakhnarovich (2009), “Random Projections.”</a></td>
                </tr>
                <tr>
                    <td>March 8th</td>
                    <td>Perceptrons</td>
                    <td>Christine and Audrey</td>
                    <td><a href="https://web.mit.edu/course/other/i2course/www/vision_and_learning/perceptron_notes.pdf">H. Sompolinsky, "Perceptron."</a>
                        <br><a href="https://www.cs.columbia.edu/~verma/classes/ml/lec/lec3_linperceptron_kern.pdf">Kernelization of Perceptrons (Slides 28-34, Nakul Verma).</a></td>
                </tr>
                <tr>
                    <td>March 15th</td>
                    <td>Break</td>
                    <td></td>
                    <td></td>
                </tr>
                <tr>
                    <td>March 22nd</td>
                    <td>Kernels and Similarity Matrices</td>
                    <td>Shujun</td>
                    <td>
                        <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5421820/">J. Diedrichsen & N. Kriegeskorte (2017), "Representational Models: A Common Framework for Understanding Encoding, Pattern-component, and Representational-similarity Analysis.”</a>
                        <br><a href="https://arxiv.org/abs/1806.07572">A. Jacot, et al (2018), "Neural Tangent Kernel: Convergence and Generalization in Neural Networks.”</a>
                    </td>
                </tr>
                <tr>
                    <td>March 29th</td>
                    <td>Physics of Capacity</td>
                    <td>Shujun</td>
                    <td>
                        <a href="https://iopscience.iop.org/article/10.1088/0305-4470/21/1/030">E. Gardner (1988), "The Space of Interactions in Neural Network Models."</a>
                    </td>
                </tr>
                <tr>
                    <td>April 5th</td>
                    <td>Manifolds</td>
                    <td>Shujun</td>
                    <td>
                        <a href="https://journals.aps.org/prx/pdf/10.1103/PhysRevX.8.031003">S. Chung et al (2018), "Classification and Geometry of General Perceptual Manifolds.”</a>
                        <br><a href="https://arxiv.org/abs/2104.07059">Chung S, Abbott LF (2021). “Neural population geometry: An approach for understanding biological and artificial neural networks.”</a>
                        <br><a href="https://core.ac.uk/reader/82789300">Kriegeskorte N, Kievit RA (2013), “Representational geometry: integrating cognition, computation, and the brain.”</a>
                        <br><a href="https://www.frontiersin.org/articles/10.3389/fncom.2023.1197031/full">X. Li and S. Wang (2023), “Toward a computational theory of manifold untangling: from global embedding to local flattening”</a>
                    </td>
                </tr>
                <tr>
                    <td>April 12th</td>
                    <td>Generalisation Error Theory</td>
                    <td>Shujun</td>
                    <td>
                        <a href="https://www.cs.huji.ac.il/labs/learning/Papers/SST92.pdf">H.S. Seung et al (1992), "Statistical Mechanics of Learning from Examples."</a>
                        <br><a href="https://pehlevan.seas.harvard.edu/files/pehlevan/files/canatar_etal_natcom_2021.pdf">A. Canatar et al (2021), "Spectral Bias and Task-model Alignment Explain Generalization in Kernel Regression and Infinitely Wide Neural Networks."</a>
                    </td>
                </tr>
                <tr>
                    <td>April 19th</td>
                    <td>Deep Learning in Neuroscience: A Look to the Future</td>
                    <td>Shujun</td>
                    <td>
                        <a href="https://d1wqtxts1xzle7.cloudfront.net/43799961/yamins_dicarlo_NN_2016-libre.pdf?1458178368=&response-content-disposition=inline%3B+filename%3DUsing_goal_driven_deep_learning_models_t.pdf&Expires=1708660109&Signature=LCuG0w2RgmaRFGA60x2nAtGYKIZGH5iECutcdLSaqE8a4XtMHusX2b14tzrfj0-Gt0O2jKUjVyEyqvOKhrGz8dE8ZbH-9xJsEi9rx656Zz6Cf11qzaYnIZREJAEVX4OQ5NC1OU9hF~gfOF48nQTwqeAPqAcIZuBzZ7rPAR4ZOFB94G31BNj3Y9A-T4-twW5kNaETNwN3TcBjvyQrCVVqneSrugqNeu8cCevPs53iMdy6XjWrVLIywsxx1RbTnS7wkXWuJWy7vgSxCZ5bg1fhtfYH~Q~Y5vbB6tm7Avf7WUek5nJEfmsK80K6xSXxCKtG3gyVAJmcto7xZIYPlguSOg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">D.L. Yamins & J.J. DiCarlo (2016), "Using Goal-driven Deep Learning Models to Understand Sensory Cortex.”</a>
                        <br><a href="https://discovery.ucl.ac.uk/id/eprint/10086844/1/A%20deep%20learning%20framework%20for%20neuroscience%20-%20vFinalv2.pdf">B.A. Richards et al. (2019), "A Deep Learning Framework for Neuroscience.”</a>
                        <br><a href="https://www.nature.com/articles/s41583-020-00395-8">A. Saxe, et al (2021), "If Deep Learning Is the Answer, What Is the Question?"</a>
                        <br><a href="https://www.nature.com/articles/s41583-020-0277-3">Lillicrap TP, et al (2020), “Backpropagation and the brain.”</a>
                    </td>
                </tr>
            </table>
        </div>

        <div class='spring24-group'>
            <h3>Quantum Complexity</h3>
            <p>Organizers: Mark and Ryan. Graduate Student Mentor: Natalie.</p>
            <p><b>Description:</b> This seminar aims to explore fundamental questions about the relationship of quantum computation to classical computation through the lens of complexity theory, looking high and low for answers. We will cover quantum complexity classes such as BQP and QMA, the classical simulation of quantum circuits, Hamiltonian complexity, and classical-quantum separations for constant-depth circuits.</p>
            <div style="height: 10px;"></div>
            <table>
                <tr>
                    <th>Resource</th>
                    <th>Title</th>
                    <th>Link</th>
                </tr>
                <tr>
                    <td>Grier</td>
                    <td>Quantum Complexity Theory</td>
                    <td><a href="https://danielgrier.com/courses/CSE291/Fa22/">Link</a></td>
                </tr>
                <tr>
                    <td>DeWolf</td>
                    <td>Quantum Computing Notes</td>
                    <td><a href="https://danielgrier.com/courses/CSE291/Fa22/">Link</a></td>
                </tr>
                <tr>
                    <td>Yuen</td>
                    <td>Advanced Topics in Quantum Information Theory</td>
                    <td><a href="https://henryyuen.net/classes/fall2020/">Link</a></td>
                </tr>
            </table>
            <div style="height: 30px;"></div>
            <table>
                <tr>
                    <th>Date</th>
                    <th>Topic</th>
                    <th>Speaker</th>
                    <th>Reading</th>
                </tr>
                <tr>
                    <td>February 23rd</td>
                    <td>Introduction and Overview</td>
                    <td>Ryan and Mark</td>
                    <td><a href="https://www.cs.cmu.edu/~odonnell/quantum15/lecture01.pdf">[O'Donnell, Wright] Lecture 01 -- Introduction to the Quantum Circuit Model.</a></td>
                </tr>
                <tr>
                    <td>March 1st</td>
                    <td>Quantum complexity theory: Most functions are hard to compute, BPP ⊆ BQP ⊆ PSPACE</td>
                    <td>Shiv</td>
                    <td>DeWolf, Chapter 13</td>
                </tr>
                <tr>
                    <td>March 8th</td> 
                    <td>Hardness of simulating depth-3 quantum circuits</td> 
                    <td>Tyler</td> 
                    <td>Chapter 2.5 of <a href="https://uwspace.uwaterloo.ca/bitstream/handle/10012/18702/Parham_Natalie.pdf?sequence=5&isAllowed=y">N. Parham (2022)</a></td> 
                </tr> 
                <tr>
                    <td>March 15th</td> 
                    <td>Break</td> 
                    <td></td> 
                    <td></td> 
                </tr>
                <tr>
                    <td>March 22nd</td>
                    <td>Efficient classical simulation of clifford circuits</td>
                    <td>Christine</td>
                    <td>Grier, Lecture 11</td>
                </tr>
                <tr>
                    <td>March 29th</td>
                    <td>NYC Quantum Algorithms, Complexity and Cryptography (QuACC) Day</td> 
                    <td>Natalie</td> 
                    <td></td> 
                </tr>
                <tr>
                    <td>April 5th</td>
                    <td>Bravyi-Gosset-König separation between constant-depth quantum and classical circuits</td>
                    <td>Zara</td>
                    <td>DeWolf, Chapter 14<br>Yuen, Lecture 3</td>
                </tr>
                <tr>
                    <td>April 12th</td>
                    <td>QMA and the local Hamiltonian problem.</td>
                    <td>Andrew Yang, Mark</td>
                    <td>Yuen, Lecture 4</td>
                </tr>
                <tr>
                    <td>April 26th</td>
                    <td>The quantum PCP conjecture.</td>
                    <td>Alan</td>
                    <td>Yuen, Lecture 5</td>
                </tr>
            </table>
        </div>
        <div class='spring24-group'>
            <h3>Randomness in Computation</h3>
            <p>Organizer: Ekene.</p>
            <p><b>Description:</b> In this group, we'll learn what it means to include randomness as a resource for computation. We'll look at many randomized algorithms for famous problems, look at a complexity-theoretic approach to randomness, and also discuss probabilistic proofs and methods for obtaining deterministic algorithms from existing random ones. Some background knowledge on probability is helpful but not necessary.</p>
            <div style="height: 10px;"></div>
            <table>
                <tr>
                    <th>Resource</th>
                    <th>Title</th>
                    <th>Link</th>
                </tr>
                <tr>
                    <td>Motwani and Raghavan</td>
                    <td>Randomised Algorithms</td>
                    <td><a href="https://rajsain.files.wordpress.com/2013/11/randomized-algorithms-motwani-and-raghavan.pdf">Link</a></td>
                </tr>
                <tr>
                    <td>Mitzenmacher and Upfal</td>
                    <td>Probability and Computing</td>
                    <td><a href="https://www.cs.purdue.edu/homes/spa/courses/pg17/mu-book.pdf">Link</a></td>
                </tr>
                <tr>
                    <td>Goldreich</td>
                    <td>Randomness in Computation</td>
                    <td><a href="https://www.wisdom.weizmann.ac.il/~oded/PS/r+c.pdf">Link</a></td>
                </tr>
            </table>
            <div style="height: 30px;"></div>
            <table>
                <tr>
                    <th>Date</th>
                    <th>Topic</th>
                    <th>Speaker</th>
                    <th>Reading</th>
                </tr>
                <tr>
                    <td>February 17th</td>
                    <td>Introduction, Las Vegas and Monte Carlo algorithms</td>
                    <td>Ekene</td>
                    <td>Chapter 1, Motwani and Raghavan; Chapters 1 and 2, Mitzenmacher and Upfal; Section 1 of Goldreich</td>
                </tr>
                <tr>
                    <td>February 24th</td>
                    <td>Moments, Deviations, and Concentration Inequalities</td>
                    <td>Nikhil</td>
                    <td>Chapters 3 and 4 of Motwani and Raghavan, Mitzenmacher and Upfal</td>
                </tr>
                <tr>
                    <td>March 2nd</td>
                    <td>The Probabilistic Method</td>
                    <td>Lucas</td>
                    <td>Chapter 5 of Motwani and Raghavan; Chapter 6 of Mitzenmacher and Upfal</td>
                </tr>
                <tr>
                    <td>March 9th</td>
                    <td>Random Walks and Markov Chains</td>
                    <td>Mary</td>
                    <td>Chapter 6 of Motwani and Raghavan; Chapter 7 of Mitzenmacher and Upfal</td>
                </tr>
                <tr>
                    <td>March 16th</td>
                    <td>Approximate Counting, Jerrum-Sinclair-Vigoda</td>
                    <td>Ekene</td>
                    <td>Chapter 11 of Motwani and Raghavan; <a href="https://faculty.cc.gatech.edu/~vigoda/Permanent.pdf">Jerrum, Sinclair, Vigoda (2004)</a></td>
                </tr>
                <tr>
                    <td>March 23rd</td>
                    <td>Pseudorandomness and Derandomisation</td>
                    <td>Ekene</td>
                    <td>Section 2 of Goldreich</td>
                </tr>
                <tr>
                    <td>March 30th</td>
                    <td>Randomness in Sequential Computation</td>
                    <td>Lisa</td>
                    <td>Chapters 8 and 10 of Motwani and Raghavan</td>
                </tr>
                <tr>
                    <td>April 6th</td>
                    <td>Probabilistic Proof Systems</td>
                    <td>Niko</td>
                    <td>Section 3 of Goldreich</td>
                </tr>
                <tr>
                    <td>April 13th</td>
                    <td>Sublinear-time Algorithms with Randomness</td>
                    <td>Ekene</td>
                    <td>Section 5 of Goldreich</td>
                </tr>
            </table>
        </div>
        
    <hr>
    <h2>Previous Semesters</h2>
    <h2>
        <a href="fall2023.html">Fall 2023</a>
    </h2>
    <h2>
        <a href="spring2023.html">Spring 2023</a>    
    <h2>
    <h2>
        <a href="fall2022.html">Fall 2022</a>    
    <h2>
    <h2>
        <a href="summer2022.html">Summer 2022</a>    
    <h2>
    <h2>
        <a href="spring2022.html">Spring 2022</a>    
    <h2>
        <a href="fall2021.html">Fall 2021</a>
    </h2>
    <h2>
        <a href="summer2021.html">Summer 2021</a>
    </h2>
</body>
